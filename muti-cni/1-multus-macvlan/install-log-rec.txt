
#kubectl apply -f calico.yaml

kubectl apply -f ./multus-cni/deployments/multus-daemonset.yml
customresourcedefinition.apiextensions.k8s.io/network-attachment-definitions.k8s.cni.cncf.io unchanged
clusterrole.rbac.authorization.k8s.io/multus unchanged
clusterrolebinding.rbac.authorization.k8s.io/multus unchanged
serviceaccount/multus unchanged
configmap/multus-cni-config unchanged
daemonset.apps/kube-multus-ds unchanged

kubectl apply -f ./whereabouts/doc/crds/
serviceaccount/whereabouts unchanged
clusterrolebinding.rbac.authorization.k8s.io/whereabouts unchanged
clusterrole.rbac.authorization.k8s.io/whereabouts-cni unchanged
daemonset.apps/whereabouts unchanged
customresourcedefinition.apiextensions.k8s.io/ippools.whereabouts.cni.cncf.io configured
customresourcedefinition.apiextensions.k8s.io/overlappingrangeipreservations.whereabouts.cni.cncf.io configured

kubectl get nodes -o wide
NAME   STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
bpf1   Ready    control-plane,master   9d    v1.23.4   192.168.2.71   <none>        Ubuntu 20.04.5 LTS   5.15.0-52-generic   docker://20.10.21
bpf2   Ready    <none>                 9d    v1.23.4   192.168.2.72   <none>        Ubuntu 20.04.5 LTS   5.15.0-52-generic   docker://20.10.21
bpf3   Ready    <none>                 9d    v1.23.4   192.168.2.73   <none>        Ubuntu 20.04.5 LTS   5.15.0-52-generic   docker://20.10.21

kubectl get pods -o wide -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS        AGE     IP               NODE   NOMINATED NODE   READINESS GATES
default       calico-bgp-fullmesh-b7fgg                  1/1     Running   0               4h56m   10.244.230.1     bpf2   <none>           <none>
default       calico-bgp-fullmesh-ndn9h                  1/1     Running   0               4h56m   10.244.179.130   bpf3   <none>           <none>
default       calico-bgp-fullmesh-pwhq4                  1/1     Running   0               4h56m   10.244.11.64     bpf1   <none>           <none>
kube-system   calico-kube-controllers-7d77d66bd7-ddhhj   1/1     Running   0               4h56m   10.244.179.129   bpf3   <none>           <none>
kube-system   calico-node-5l7k7                          1/1     Running   0               4h56m   192.168.2.73     bpf3   <none>           <none>
kube-system   calico-node-d77p7                          1/1     Running   0               4h56m   192.168.2.71     bpf1   <none>           <none>
kube-system   calico-node-gcgbg                          1/1     Running   0               4h56m   192.168.2.72     bpf2   <none>           <none>
kube-system   coredns-7b888c7f44-knf7l                   1/1     Running   1 (7d7h ago)    7d8h    10.244.230.0     bpf2   <none>           <none>
kube-system   coredns-7b888c7f44-vhg82                   1/1     Running   1 (7d7h ago)    7d8h    10.244.179.128   bpf3   <none>           <none>
kube-system   etcd-bpf1                                  1/1     Running   10 (7d7h ago)   9d      192.168.2.71     bpf1   <none>           <none>
kube-system   kube-apiserver-bpf1                        1/1     Running   11 (7d7h ago)   9d      192.168.2.71     bpf1   <none>           <none>
kube-system   kube-controller-manager-bpf1               1/1     Running   10 (7d7h ago)   9d      192.168.2.71     bpf1   <none>           <none>
kube-system   kube-multus-ds-2vdrz                       1/1     Running   0               73m     192.168.2.71     bpf1   <none>           <none>
kube-system   kube-multus-ds-vg7wj                       1/1     Running   0               73m     192.168.2.73     bpf3   <none>           <none>
kube-system   kube-multus-ds-zv4j2                       1/1     Running   0               73m     192.168.2.72     bpf2   <none>           <none>
kube-system   kube-proxy-7zkvw                           1/1     Running   6 (7d7h ago)    9d      192.168.2.72     bpf2   <none>           <none>
kube-system   kube-proxy-pl2pk                           1/1     Running   7 (7d7h ago)    9d      192.168.2.73     bpf3   <none>           <none>
kube-system   kube-proxy-wmv2v                           1/1     Running   10 (7d7h ago)   9d      192.168.2.71     bpf1   <none>           <none>
kube-system   kube-scheduler-bpf1                        1/1     Running   12 (7d7h ago)   9d      192.168.2.71     bpf1   <none>           <none>
kube-system   whereabouts-hqkm2                          1/1     Running   0               76m     192.168.2.72     bpf2   <none>           <none>
kube-system   whereabouts-hrmnk                          1/1     Running   0               76m     192.168.2.73     bpf3   <none>           <none>
kube-system   whereabouts-svlqq                          1/1     Running   0               76m     192.168.2.71     bpf1   <none>           <none>

cat <<EOF | kubectl apply -f -
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: macvlan-whereabouts-conf
spec:
  config: '{
      "cniVersion": "0.3.0",
      "name": "whereaboutsexample",
      "type": "macvlan",
      "master": "ens160",
      "mode": "bridge",
      "ipam": {
        "type": "whereabouts",
        "range": "192.168.2.200-192.168.2.205/24"
      }
    }'
EOF
networkattachmentdefinition.k8s.cni.cncf.io/macvlan-whereabouts-conf unchanged

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: muti-cni-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: macvlan-whereabouts-conf@net1
spec:
  containers:
  - name: nettool
    image: 192.168.2.100:5000/nettool
    securityContext:
      privileged: false
      capabilities:
        add: ["NET_ADMIN"]
EOF
pod/muti-cni-pod created

sleep 5

kubectl exec -it muti-cni-pod -- ifconfig -a
eth0      Link encap:Ethernet  HWaddr 6E:C0:EC:D0:18:EB  
          inet addr:10.244.230.6  Bcast:0.0.0.0  Mask:255.255.255.255
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:14 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:1886 (1.8 KiB)  TX bytes:0 (0.0 B)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)

net1      Link encap:Ethernet  HWaddr 42:99:0A:25:32:BC  
          inet addr:192.168.2.200  Bcast:192.168.2.255  Mask:255.255.255.0
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:0 (0.0 B)  TX bytes:42 (42.0 B)


kubectl exec -it muti-cni-pod -- ip route add 0.0.0.0/0 via 192.168.2.1 table 100
kubectl exec -it muti-cni-pod -- ip rule add from 192.168.2.0/24 table 100

kubectl exec -it muti-cni-pod -- bash -c "ping -c 1 114.114.114.114 -I 192.168.2.200"
PING 114.114.114.114 (114.114.114.114) from 192.168.2.200: 56 data bytes
64 bytes from 114.114.114.114: seq=0 ttl=74 time=17.072 ms

--- 114.114.114.114 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 17.072/17.072/17.072 ms

kubectl exec -it muti-cni-pod -- ip rule s
0:	from all lookup local
32765:	from 192.168.2.0/24 lookup 100
32766:	from all lookup main
32767:	from all lookup default

kubectl exec -it muti-cni-pod -- ip r s t 100
default via 192.168.2.1 dev net1 
